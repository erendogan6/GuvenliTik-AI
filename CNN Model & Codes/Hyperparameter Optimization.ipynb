{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0999bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Importing the numpy library, often used for numerical operations on arrays and matrices.\n",
    "\n",
    "import tensorflow as tf\n",
    "# Importing TensorFlow, a popular machine learning library.\n",
    "\n",
    "import pandas as pd\n",
    "# Importing the pandas library, used for data manipulation and analysis.\n",
    "\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "# Importing BatchNormalization and Dropout layers from Keras. These are used in neural network layers.\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# Importing the Tokenizer class from TensorFlow Keras, used for text tokenization.\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# Importing pad_sequences from TensorFlow Keras, used to ensure all sequences in a list have the same length.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Importing the Sequential model from TensorFlow Keras, a linear stack of neural network layers.\n",
    "\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
    "# Importing various types of layers from TensorFlow Keras used in building neural networks.\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Importing to_categorical from TensorFlow Keras, used for converting class vectors to binary class matrices.\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Importing LabelEncoder from Scikit-Learn, used for encoding labels with value between 0 and n_classes-1.\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "# Importing EarlyStopping and ModelCheckpoint callbacks from TensorFlow Keras, used to monitor training.\n",
    "\n",
    "from itertools import product\n",
    "# Importing product from itertools, used to create Cartesian products from multiple iterables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d0d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load datasets and handle errors\n",
    "def load_dataset_skip_errors(file_path, sample_fraction=0.1):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, header=None, quotechar='\"')  # Read the CSV file without a header and with a specified quote character.\n",
    "        df_sampled = df.sample(frac=sample_fraction, random_state=1)  # Randomly sample a fraction of the dataframe.\n",
    "        return df_sampled\n",
    "    except pd.errors.ParserError as e:\n",
    "        print(f'ParserError: {e}')  # Print and handle any parser errors.\n",
    "        return None\n",
    "\n",
    "# Load the training and testing datasets\n",
    "train_df = load_dataset_skip_errors(\"updated_train.csv\")  # Load and sample the training dataset.\n",
    "test_df = load_dataset_skip_errors(\"updated_test.csv\")  # Load and sample the testing dataset.\n",
    "\n",
    "# Validate that the datasets are loaded\n",
    "if train_df is None or test_df is None:\n",
    "    raise ValueError(\"DataFrames could not be loaded. Check your CSV files.\")  # Check if datasets are loaded successfully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d15e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare URL and label data\n",
    "train_urls = train_df[1].tolist()  # Extract URLs from the training dataset.\n",
    "train_labels = train_df[0].tolist()  # Extract labels from the training dataset.\n",
    "test_urls = test_df[1].tolist()  # Extract URLs from the testing dataset.\n",
    "test_labels = test_df[0].tolist()  # Extract labels from the testing dataset.\n",
    "\n",
    "# Tokenize URLs\n",
    "tokenizer = Tokenizer(char_level=True)  # Initialize a tokenizer for character-level tokenization.\n",
    "tokenizer.fit_on_texts(train_urls)  # Fit the tokenizer on the training URLs.\n",
    "train_sequences = tokenizer.texts_to_sequences(train_urls)  # Convert training URLs to sequences of integers.\n",
    "test_sequences = tokenizer.texts_to_sequences(test_urls)  # Convert testing URLs to sequences of integers.\n",
    "\n",
    "# Padding sequences\n",
    "max_length = max(max(len(s) for s in train_sequences), max(len(s) for s in test_sequences))  # Determine the maximum sequence length.\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_length)  # Pad training sequences to the same length.\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_length)  # Pad testing sequences to the same length.\n",
    "\n",
    "# Label encoding and one-hot encoding\n",
    "label_encoder = LabelEncoder()  # Initialize the label encoder.\n",
    "train_labels = label_encoder.fit_transform(train_labels)  # Fit and transform training labels to normalized encoding.\n",
    "test_labels = label_encoder.transform(test_labels)  # Transform testing labels to normalized encoding.\n",
    "num_classes = np.max(train_labels) + 1  # Determine the number of classes.\n",
    "train_labels = to_categorical(train_labels, num_classes)  # Convert training labels to one-hot encoding.\n",
    "test_labels = to_categorical(test_labels, num_classes)  # Convert testing labels to one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93093bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter optimization\n",
    "param_grid = {\n",
    "    'optimizer':['adam','rmsprop'],  # Define optimizer types to test.\n",
    "    'filters': [32,64,128,256],  # Define filter sizes for Conv1D layers to test.\n",
    "    'kernel_size': [3,6],  # Define kernel sizes for Conv1D layers to test.\n",
    "    'batch_size': [32],  # Define batch sizes to test.\n",
    "    'epochs': [20],  # Define number of epochs to test.\n",
    "    'num_layers': [1, 2, 3],  # Define number of layers to test.\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3aecbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search function for hyperparameter tuning\n",
    "def grid_search(param_grid, train_data, train_labels, test_data, test_labels):\n",
    "    best_accuracy = 0.0  # Initialize the best accuracy.\n",
    "    best_params = {}  # Initialize the dictionary to store best parameters.\n",
    "    results = []  # List to store results of each parameter combination.\n",
    "\n",
    "    keys = param_grid.keys()  # Get the keys (parameter names) from the parameter grid.\n",
    "    param_combinations = list(product(*param_grid.values()))  # Generate all combinations of parameters.\n",
    "\n",
    "    # Initialize early stopping to avoid overfitting.\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        patience=4,\n",
    "        min_delta=0.0010,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # Iterate through each parameter combination.\n",
    "    for params in param_combinations:\n",
    "        params_dict = dict(zip(keys, params))  # Create a dictionary of the current parameter combination.\n",
    "        model = Sequential()  # Initialize a sequential model.\n",
    "        # Add an embedding layer.\n",
    "        model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=50, input_length=max_length))\n",
    "\n",
    "        # Add the specified number of Conv1D layers.\n",
    "        for _ in range(params_dict['num_layers']):\n",
    "            model.add(Conv1D(filters=params_dict['filters'], kernel_size=params_dict['kernel_size'], activation='relu'))\n",
    "            model.add(BatchNormalization())  # Add batch normalization.\n",
    "            model.add(Dropout(0.2))  # Add dropout for regularization.\n",
    "\n",
    "        # Add the final layers.\n",
    "        model.add(GlobalMaxPooling1D())  # Add global max pooling layer.\n",
    "        model.add(Dense(num_classes, activation='sigmoid'))  # Add a dense output layer with sigmoid activation.\n",
    "        \n",
    "        # Compile the model.\n",
    "        model.compile(optimizer=params_dict['optimizer'], loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        model.summary()  # Print the model summary.\n",
    "\n",
    "        # Train the model.\n",
    "        model.fit(train_data, train_labels, epochs=params_dict['epochs'], validation_data=(test_data, test_labels),\n",
    "                  batch_size=params_dict['batch_size'], verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "        # Evaluate the model.\n",
    "        _, accuracy = model.evaluate(test_data, test_labels, verbose=1)\n",
    "\n",
    "        # Store the results.\n",
    "        result = {'params': params_dict, 'accuracy': accuracy}\n",
    "        results.append(result)\n",
    "        print(f\"Parameters: {params_dict}, Accuracy: {accuracy}\")\n",
    "\n",
    "        # Update the best accuracy and parameters if current accuracy is better.\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_params = params_dict\n",
    "\n",
    "    return best_params, best_accuracy, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa12719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grid search.\n",
    "best_params, best_accuracy, all_results = grid_search(param_grid, train_data, train_labels, test_data, test_labels)\n",
    "print(\"Best parameters:\", best_params)  # Print the best parameters.\n",
    "print(\"Best accuracy:\", best_accuracy)  # Print the best accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
